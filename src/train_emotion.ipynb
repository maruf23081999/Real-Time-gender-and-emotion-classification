{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 64, 64, 1)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)                (None, 62, 62, 8)     72          input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNorm (None, 62, 62, 8)     32          conv2d_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 62, 62, 8)     0           batch_normalization_1[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)                (None, 60, 60, 8)     576         activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNorm (None, 60, 60, 8)     32          conv2d_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 60, 60, 8)     0           batch_normalization_2[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "separable_conv2d_1 (SeparableCon (None, 60, 60, 16)    200         activation_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNorm (None, 60, 60, 16)    64          separable_conv2d_1[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, 60, 60, 16)    0           batch_normalization_4[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "separable_conv2d_2 (SeparableCon (None, 60, 60, 16)    400         activation_3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNorm (None, 60, 60, 16)    64          separable_conv2d_2[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)                (None, 30, 30, 16)    128         activation_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)   (None, 30, 30, 16)    0           batch_normalization_5[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNorm (None, 30, 30, 16)    64          conv2d_3[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "add_1 (Add)                      (None, 30, 30, 16)    0           max_pooling2d_1[0][0]            \n",
      "                                                                   batch_normalization_3[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "separable_conv2d_3 (SeparableCon (None, 30, 30, 32)    656         add_1[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNorm (None, 30, 30, 32)    128         separable_conv2d_3[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "activation_4 (Activation)        (None, 30, 30, 32)    0           batch_normalization_7[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "separable_conv2d_4 (SeparableCon (None, 30, 30, 32)    1312        activation_4[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNorm (None, 30, 30, 32)    128         separable_conv2d_4[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)                (None, 15, 15, 32)    512         add_1[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)   (None, 15, 15, 32)    0           batch_normalization_8[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNorm (None, 15, 15, 32)    128         conv2d_4[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "add_2 (Add)                      (None, 15, 15, 32)    0           max_pooling2d_2[0][0]            \n",
      "                                                                   batch_normalization_6[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "separable_conv2d_5 (SeparableCon (None, 15, 15, 64)    2336        add_2[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNor (None, 15, 15, 64)    256         separable_conv2d_5[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "activation_5 (Activation)        (None, 15, 15, 64)    0           batch_normalization_10[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "separable_conv2d_6 (SeparableCon (None, 15, 15, 64)    4672        activation_5[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNor (None, 15, 15, 64)    256         separable_conv2d_6[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)                (None, 8, 8, 64)      2048        add_2[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)   (None, 8, 8, 64)      0           batch_normalization_11[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNorm (None, 8, 8, 64)      256         conv2d_5[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "add_3 (Add)                      (None, 8, 8, 64)      0           max_pooling2d_3[0][0]            \n",
      "                                                                   batch_normalization_9[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "separable_conv2d_7 (SeparableCon (None, 8, 8, 128)     8768        add_3[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNor (None, 8, 8, 128)     512         separable_conv2d_7[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "activation_6 (Activation)        (None, 8, 8, 128)     0           batch_normalization_13[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "separable_conv2d_8 (SeparableCon (None, 8, 8, 128)     17536       activation_6[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNor (None, 8, 8, 128)     512         separable_conv2d_8[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)                (None, 4, 4, 128)     8192        add_3[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)   (None, 4, 4, 128)     0           batch_normalization_14[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNor (None, 4, 4, 128)     512         conv2d_6[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "add_4 (Add)                      (None, 4, 4, 128)     0           max_pooling2d_4[0][0]            \n",
      "                                                                   batch_normalization_12[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)                (None, 4, 4, 7)       8071        add_4[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glob (None, 7)             0           conv2d_7[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "predictions (Activation)         (None, 7)             0           global_average_pooling2d_1[0][0] \n",
      "====================================================================================================\n",
      "Total params: 58,423\n",
      "Trainable params: 56,951\n",
      "Non-trainable params: 1,472\n",
      "____________________________________________________________________________________________________\n",
      "Training dataset: fer2013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "897/897 [============================>.] - ETA: 0s - loss: 1.7830 - acc: 0.3213Epoch 00000: val_loss improved from inf to 1.57692, saving model to ../trained_models/emotion_models/fer2013_mini_XCEPTION.00-0.41.hdf5\n",
      "898/897 [==============================] - 871s - loss: 1.7837 - acc: 0.3209 - val_loss: 1.5769 - val_acc: 0.4093\n",
      "Epoch 2/30\n",
      "897/897 [============================>.] - ETA: 0s - loss: 1.4996 - acc: 0.4404Epoch 00001: val_loss improved from 1.57692 to 1.47875, saving model to ../trained_models/emotion_models/fer2013_mini_XCEPTION.01-0.45.hdf5\n",
      "898/897 [==============================] - 819s - loss: 1.4995 - acc: 0.4405 - val_loss: 1.4788 - val_acc: 0.4483\n",
      "Epoch 3/30\n",
      "897/897 [============================>.] - ETA: 0s - loss: 1.3774 - acc: 0.4871Epoch 00002: val_loss improved from 1.47875 to 1.43816, saving model to ../trained_models/emotion_models/fer2013_mini_XCEPTION.02-0.45.hdf5\n",
      "898/897 [==============================] - 852s - loss: 1.3764 - acc: 0.4876 - val_loss: 1.4382 - val_acc: 0.4540\n",
      "Epoch 4/30\n",
      "897/897 [============================>.] - ETA: 0s - loss: 1.3121 - acc: 0.5106Epoch 00003: val_loss improved from 1.43816 to 1.41827, saving model to ../trained_models/emotion_models/fer2013_mini_XCEPTION.03-0.50.hdf5\n",
      "898/897 [==============================] - 829s - loss: 1.3123 - acc: 0.5105 - val_loss: 1.4183 - val_acc: 0.4967\n",
      "Epoch 5/30\n",
      "897/897 [============================>.] - ETA: 0s - loss: 1.2612 - acc: 0.5288Epoch 00004: val_loss improved from 1.41827 to 1.30112, saving model to ../trained_models/emotion_models/fer2013_mini_XCEPTION.04-0.51.hdf5\n",
      "898/897 [==============================] - 818s - loss: 1.2613 - acc: 0.5291 - val_loss: 1.3011 - val_acc: 0.5134\n",
      "Epoch 6/30\n",
      "897/897 [============================>.] - ETA: 0s - loss: 1.2225 - acc: 0.5410Epoch 00005: val_loss improved from 1.30112 to 1.18760, saving model to ../trained_models/emotion_models/fer2013_mini_XCEPTION.05-0.55.hdf5\n",
      "898/897 [==============================] - 814s - loss: 1.2225 - acc: 0.5411 - val_loss: 1.1876 - val_acc: 0.5521\n",
      "Epoch 7/30\n",
      "897/897 [============================>.] - ETA: 0s - loss: 1.2081 - acc: 0.5478Epoch 00006: val_loss did not improve\n",
      "898/897 [==============================] - 814s - loss: 1.2087 - acc: 0.5474 - val_loss: 1.2170 - val_acc: 0.5474\n",
      "Epoch 8/30\n",
      "897/897 [============================>.] - ETA: 0s - loss: 1.1772 - acc: 0.5603Epoch 00007: val_loss did not improve\n",
      "898/897 [==============================] - 825s - loss: 1.1771 - acc: 0.5603 - val_loss: 1.2853 - val_acc: 0.5171\n",
      "Epoch 9/30\n",
      "897/897 [============================>.] - ETA: 0s - loss: 1.1655 - acc: 0.5665Epoch 00008: val_loss did not improve\n",
      "898/897 [==============================] - 910s - loss: 1.1659 - acc: 0.5663 - val_loss: 1.1998 - val_acc: 0.5521\n",
      "Epoch 10/30\n",
      "897/897 [============================>.] - ETA: 0s - loss: 1.1448 - acc: 0.5718Epoch 00009: val_loss improved from 1.18760 to 1.17846, saving model to ../trained_models/emotion_models/fer2013_mini_XCEPTION.09-0.56.hdf5\n",
      "898/897 [==============================] - 942s - loss: 1.1445 - acc: 0.5718 - val_loss: 1.1785 - val_acc: 0.5603\n",
      "Epoch 11/30\n",
      "897/897 [============================>.] - ETA: 0s - loss: 1.1273 - acc: 0.5810Epoch 00010: val_loss improved from 1.17846 to 1.13511, saving model to ../trained_models/emotion_models/fer2013_mini_XCEPTION.10-0.58.hdf5\n",
      "898/897 [==============================] - 845s - loss: 1.1275 - acc: 0.5810 - val_loss: 1.1351 - val_acc: 0.5809\n",
      "Epoch 12/30\n",
      "897/897 [============================>.] - ETA: 0s - loss: 1.1180 - acc: 0.5837Epoch 00011: val_loss did not improve\n",
      "898/897 [==============================] - 835s - loss: 1.1181 - acc: 0.5837 - val_loss: 1.2034 - val_acc: 0.5605\n",
      "Epoch 13/30\n",
      "897/897 [============================>.] - ETA: 0s - loss: 1.1084 - acc: 0.5843Epoch 00012: val_loss did not improve\n",
      "898/897 [==============================] - 960s - loss: 1.1087 - acc: 0.5841 - val_loss: 1.1637 - val_acc: 0.5704\n",
      "Epoch 14/30\n",
      "897/897 [============================>.] - ETA: 0s - loss: 1.0982 - acc: 0.5899Epoch 00013: val_loss did not improve\n",
      "898/897 [==============================] - 1141s - loss: 1.0993 - acc: 0.5897 - val_loss: 1.1371 - val_acc: 0.5770\n",
      "Epoch 15/30\n",
      "897/897 [============================>.] - ETA: 0s - loss: 1.0922 - acc: 0.5931Epoch 00014: val_loss improved from 1.13511 to 1.09536, saving model to ../trained_models/emotion_models/fer2013_mini_XCEPTION.14-0.59.hdf5\n",
      "898/897 [==============================] - 973s - loss: 1.0927 - acc: 0.5926 - val_loss: 1.0954 - val_acc: 0.5906\n",
      "Epoch 16/30\n",
      "897/897 [============================>.] - ETA: 0s - loss: 1.0747 - acc: 0.5977Epoch 00015: val_loss improved from 1.09536 to 1.08181, saving model to ../trained_models/emotion_models/fer2013_mini_XCEPTION.15-0.61.hdf5\n",
      "898/897 [==============================] - 1122s - loss: 1.0744 - acc: 0.5982 - val_loss: 1.0818 - val_acc: 0.6070\n",
      "Epoch 17/30\n",
      "897/897 [============================>.] - ETA: 0s - loss: 1.0745 - acc: 0.5974Epoch 00016: val_loss did not improve\n",
      "898/897 [==============================] - 1021s - loss: 1.0743 - acc: 0.5974 - val_loss: 1.1525 - val_acc: 0.5699\n",
      "Epoch 18/30\n",
      "897/897 [============================>.] - ETA: 0s - loss: 1.0659 - acc: 0.5992Epoch 00017: val_loss did not improve\n",
      "898/897 [==============================] - 899s - loss: 1.0667 - acc: 0.5985 - val_loss: 1.0919 - val_acc: 0.5945\n",
      "Epoch 19/30\n",
      "897/897 [============================>.] - ETA: 0s - loss: 1.0610 - acc: 0.6039Epoch 00018: val_loss did not improve\n",
      "898/897 [==============================] - 1034s - loss: 1.0609 - acc: 0.6038 - val_loss: 1.1411 - val_acc: 0.5822\n",
      "Epoch 20/30\n",
      "897/897 [============================>.] - ETA: 0s - loss: 1.0526 - acc: 0.6080Epoch 00019: val_loss did not improve\n",
      "898/897 [==============================] - 937s - loss: 1.0523 - acc: 0.6080 - val_loss: 1.1152 - val_acc: 0.5868\n",
      "Epoch 21/30\n",
      "897/897 [============================>.] - ETA: 0s - loss: 1.0471 - acc: 0.6123Epoch 00020: val_loss improved from 1.08181 to 1.07172, saving model to ../trained_models/emotion_models/fer2013_mini_XCEPTION.20-0.60.hdf5\n",
      "898/897 [==============================] - 1044s - loss: 1.0473 - acc: 0.6123 - val_loss: 1.0717 - val_acc: 0.6000\n",
      "Epoch 22/30\n",
      "897/897 [============================>.] - ETA: 0s - loss: 1.0405 - acc: 0.6109Epoch 00021: val_loss improved from 1.07172 to 1.05792, saving model to ../trained_models/emotion_models/fer2013_mini_XCEPTION.21-0.61.hdf5\n",
      "898/897 [==============================] - 946s - loss: 1.0408 - acc: 0.6107 - val_loss: 1.0579 - val_acc: 0.6057\n",
      "Epoch 23/30\n",
      "897/897 [============================>.] - ETA: 0s - loss: 1.0323 - acc: 0.6143Epoch 00022: val_loss did not improve\n",
      "898/897 [==============================] - 1012s - loss: 1.0336 - acc: 0.6141 - val_loss: 1.1189 - val_acc: 0.5868\n",
      "Epoch 24/30\n",
      "897/897 [============================>.] - ETA: 0s - loss: 1.0298 - acc: 0.6156Epoch 00023: val_loss did not improve\n",
      "898/897 [==============================] - 1167s - loss: 1.0300 - acc: 0.6153 - val_loss: 1.1028 - val_acc: 0.6084\n",
      "Epoch 25/30\n",
      "897/897 [============================>.] - ETA: 0s - loss: 1.0188 - acc: 0.6212Epoch 00024: val_loss did not improve\n",
      "898/897 [==============================] - 874s - loss: 1.0188 - acc: 0.6212 - val_loss: 1.0817 - val_acc: 0.6007\n",
      "Epoch 26/30\n",
      "897/897 [============================>.] - ETA: 0s - loss: 1.0131 - acc: 0.6222Epoch 00025: val_loss improved from 1.05792 to 1.04429, saving model to ../trained_models/emotion_models/fer2013_mini_XCEPTION.25-0.61.hdf5\n",
      "898/897 [==============================] - 812s - loss: 1.0131 - acc: 0.6224 - val_loss: 1.0443 - val_acc: 0.6112\n",
      "Epoch 27/30\n",
      "897/897 [============================>.] - ETA: 0s - loss: 1.0132 - acc: 0.6235Epoch 00026: val_loss did not improve\n",
      "898/897 [==============================] - 810s - loss: 1.0133 - acc: 0.6237 - val_loss: 1.0555 - val_acc: 0.6070\n",
      "Epoch 28/30\n",
      "897/897 [============================>.] - ETA: 0s - loss: 1.0099 - acc: 0.6230Epoch 00027: val_loss improved from 1.04429 to 1.01839, saving model to ../trained_models/emotion_models/fer2013_mini_XCEPTION.27-0.62.hdf5\n",
      "898/897 [==============================] - 810s - loss: 1.0099 - acc: 0.6230 - val_loss: 1.0184 - val_acc: 0.6220\n",
      "Epoch 29/30\n",
      "897/897 [============================>.] - ETA: 0s - loss: 1.0056 - acc: 0.6270Epoch 00028: val_loss did not improve\n",
      "898/897 [==============================] - 808s - loss: 1.0063 - acc: 0.6268 - val_loss: 1.1550 - val_acc: 0.5697\n",
      "Epoch 30/30\n",
      "897/897 [============================>.] - ETA: 0s - loss: 1.0030 - acc: 0.6257Epoch 00029: val_loss did not improve\n",
      "898/897 [==============================] - 809s - loss: 1.0032 - acc: 0.6256 - val_loss: 1.0715 - val_acc: 0.5975\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from models.cnn import mini_XCEPTION\n",
    "from utils.datasets import DataManager\n",
    "from utils.datasets import split_data\n",
    "from utils.preprocessor import preprocess_input\n",
    "\n",
    "# parameters\n",
    "batch_size = 32\n",
    "num_epochs = 30\n",
    "input_shape = (64, 64, 1)\n",
    "validation_split = .2\n",
    "verbose = 1\n",
    "num_classes = 7\n",
    "patience = 50\n",
    "base_path = '../trained_models/emotion_models/'\n",
    "\n",
    "# data generator\n",
    "data_generator = ImageDataGenerator(\n",
    "                        featurewise_center=False,\n",
    "                        featurewise_std_normalization=False,\n",
    "                        rotation_range=10,\n",
    "                        width_shift_range=0.1,\n",
    "                        height_shift_range=0.1,\n",
    "                        zoom_range=.1,\n",
    "                        horizontal_flip=True)\n",
    "\n",
    "# model parameters/compilation\n",
    "model = mini_XCEPTION(input_shape, num_classes)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "\n",
    "datasets = ['fer2013']\n",
    "for dataset_name in datasets:\n",
    "    print('Training dataset:', dataset_name)\n",
    "\n",
    "    # callbacks\n",
    "    log_file_path = base_path + dataset_name + '_emotion_training.log'\n",
    "    csv_logger = CSVLogger(log_file_path, append=False)\n",
    "    early_stop = EarlyStopping('val_loss', patience=patience)\n",
    "    reduce_lr = ReduceLROnPlateau('val_loss', factor=0.1,\n",
    "                                  patience=int(patience/4), verbose=1)\n",
    "    trained_models_path = base_path + dataset_name + '_mini_XCEPTION'\n",
    "    model_names = trained_models_path + '.{epoch:02d}-{val_acc:.2f}.hdf5'\n",
    "    model_checkpoint = ModelCheckpoint(model_names, 'val_loss', verbose=1,\n",
    "                                                    save_best_only=True)\n",
    "    callbacks = [model_checkpoint, csv_logger, early_stop, reduce_lr]\n",
    "\n",
    "    # loading dataset\n",
    "    data_loader = DataManager(dataset_name, image_size=input_shape[:2])\n",
    "    faces, emotions = data_loader.get_data()\n",
    "    faces = preprocess_input(faces)\n",
    "    num_samples, num_classes = emotions.shape\n",
    "    train_data, val_data = split_data(faces, emotions, validation_split)\n",
    "    train_faces, train_emotions = train_data\n",
    "    model.fit_generator(data_generator.flow(train_faces, train_emotions,\n",
    "                                            batch_size),\n",
    "                        steps_per_epoch=len(train_faces) / batch_size,\n",
    "                        epochs=num_epochs, verbose=1, callbacks=callbacks,\n",
    "                        validation_data=val_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
